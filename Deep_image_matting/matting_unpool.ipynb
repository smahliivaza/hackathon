{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#from matting import load_path,load_data,load_alphamatting_data,load_validation_data\n",
    "import os\n",
    "from scipy import misc\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d8eaa28700ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mdataset_BG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train_data/bg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpaths_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpaths_eps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpaths_BG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_eps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_BG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhard_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhard_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mrange_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths_alpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_path' is not defined"
     ]
    }
   ],
   "source": [
    "image_size = 320\n",
    "train_batch_size = 1\n",
    "max_epochs = 1000000\n",
    "hard_mode = False\n",
    "\n",
    "#checkpoint file path\n",
    "pretrained_model = False\n",
    "#pretrained_model = False\n",
    "test_dir = './alhpamatting'\n",
    "test_outdir = './test_predict'\n",
    "#validation_dir = '/data/gezheng/data-matting/new2/validation'\n",
    "\n",
    "#pretrained_vgg_model_path\n",
    "# model_path = './vgg16_weights.npz'\n",
    "log_dir = 'matting_log'\n",
    "\n",
    "dataset_alpha = 'train_data/alpha'\n",
    "dataset_eps = 'train_data/eps'\n",
    "dataset_BG = 'train_data/bg'\n",
    "\n",
    "paths_alpha,paths_eps,paths_BG = load_path(dataset_alpha,dataset_eps,dataset_BG,hard_mode = hard_mode)\n",
    "\n",
    "range_size = len(paths_alpha)\n",
    "print('range_size is %d' % range_size)\n",
    "#range_size/batch_size has to be int\n",
    "batchs_per_epoch = int(range_size/train_batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>!!!!!!!!!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1571d5fce4cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mb_RGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b_RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mb_trimap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGT_trimap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b_trimap'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mb_GTmatte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGT_matte_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b_GTmatte'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "index_queue = tf.train.range_input_producer(range_size, num_epochs=None,shuffle=True, seed=None, capacity=32)\n",
    "index_dequeue_op = index_queue.dequeue_many(train_batch_size, 'index_dequeue')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "image_batch = tf.placeholder(tf.float32, shape=(train_batch_size,image_size,image_size,3))\n",
    "raw_RGBs = tf.placeholder(tf.float32, shape=(train_batch_size,image_size,image_size,3))\n",
    "GT_matte_batch = tf.placeholder(tf.float32, shape = (train_batch_size,image_size,image_size,1))\n",
    "GT_trimap = tf.placeholder(tf.float32, shape = (train_batch_size,image_size,image_size,1))\n",
    "GTBG_batch = tf.placeholder(tf.float32, shape = (train_batch_size,image_size,image_size,3))\n",
    "GTFG_batch = tf.placeholder(tf.float32, shape = (train_batch_size,image_size,image_size,3))\n",
    "training = tf.placeholder(tf.bool)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tf.add_to_collection('image_batch',image_batch)\n",
    "tf.add_to_collection('GT_trimap',GT_trimap)\n",
    "tf.add_to_collection('training',training)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "en_parameters = []\n",
    "pool_parameters = []\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "b_RGB = tf.identity(image_batch,name = 'b_RGB')\n",
    "b_trimap = tf.identity(GT_trimap,name = 'b_trimap')\n",
    "b_GTmatte = tf.identity(GT_matte_batch,name = 'b_GTmatte')\n",
    "b_GTBG = tf.identity(GTBG_batch,name = 'b_GTBG')\n",
    "b_GTFG = tf.identity(GTFG_batch,name = 'b_GTFG')\n",
    "\n",
    "tf.summary.image('GT_matte_batch',b_GTmatte,max_outputs = 5)\n",
    "tf.summary.image('trimap',b_trimap,max_outputs = 5)\n",
    "tf.summary.image('raw_RGBs',raw_RGBs,max_outputs = 5)\n",
    "\n",
    "b_input = tf.concat([b_RGB,b_trimap],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        activation = torch.nn.ReLU()\n",
    "        # conv1_1  \n",
    "        conv1_1 = torch.nn.Conv2d(in_channels = 4, \n",
    "                       out_channels = 64,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv1_2\n",
    "        conv1_2 = torch.nn.Conv2d(in_channels = 64, \n",
    "                       out_channels = 64,\n",
    "                       kernel_size = [3, 3])\n",
    "        # pool1\n",
    "        pool1 = torch.nn.MaxPool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # conv2_1\n",
    "        conv2_1 = torch.nn.Conv2d(in_channels = 64, \n",
    "                       out_channels = 128,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv2_2\n",
    "        conv2_2 = torch.nn.Conv2d(in_channels = 128, \n",
    "                       out_channels = 128,\n",
    "                       kernel_size = [3, 3])\n",
    "        # pool2\n",
    "        pool2 = torch.nn.MaxPool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # conv3_1\n",
    "        conv3_1 = torch.nn.Conv2d(in_channels = 128, \n",
    "                       out_channels = 256,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv3_2\n",
    "        conv3_2 = torch.nn.Conv2d(in_channels = 256, \n",
    "                       out_channels = 256,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv3_3\n",
    "        conv3_3 = torch.nn.Conv2d(in_channels = 256, \n",
    "                       out_channels = 256,\n",
    "                       kernel_size = [3, 3])\n",
    "        # pool3\n",
    "        pool3 = torch.nn.MaxPool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # conv4_1\n",
    "        conv4_1 = torch.nn.Conv2d(in_channels = 256, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv4_2\n",
    "        conv4_2 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv4_3\n",
    "        conv4_3 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # pool4\n",
    "        pool4 = torch.nn.MaxPool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # conv5_1\n",
    "        conv5_1 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv5_2\n",
    "        conv5_2 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # conv5_3\n",
    "        conv5_3 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 512,\n",
    "                       kernel_size = [3, 3])\n",
    "        # pool5\n",
    "        pool5 = torch.nn.MaxPool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # conv6_1 \n",
    "        conv6_1 = torch.nn.Conv2d(in_channels = 512, \n",
    "                       out_channels = 4046,\n",
    "                       kernel_size = [7, 7])\n",
    "        \n",
    "        \n",
    "        # deconv6\n",
    "        deconv6 = torch.nn.ConvTranspose2d(in_channels = 4096, \n",
    "                             out_channels = 512, \n",
    "                             kernel_size = [1, 1])\n",
    "        batch_norm_6 = torch.nn.BatchNorm2d(num_features = 512)\n",
    "        # deconv5_1/unpooling\n",
    "        unpool_5 = torch.nn.MaxUnpool2d(kernel_size = [2, 2], \n",
    "                              stride = [2, 2])\n",
    "        # deconv5_2\n",
    "        deconv5_2 = torch.nn.ConvTranspose2d(in_channels = 512, \n",
    "                             out_channels = 512, \n",
    "                             kernel_size = [5, 5])\n",
    "        batch_norm_5 = torch.nn.BatchNorm2d(num_features = 512)\n",
    "        # deconv4_1/unpooling\n",
    "        unpool_4 = torch.nn.MaxUnpool2d(kernel_size = [2, 2], \n",
    "                              stride = [2, 2])\n",
    "        # deconv4_2\n",
    "        deconv4_2 = torch.nn.ConvTranspose2d(in_channels = 512, \n",
    "                             out_channels = 256, \n",
    "                             kernel_size = [5, 5])\n",
    "        batch_norm_4 = torch.nn.BatchNorm2d(num_features = 256)\n",
    "        # deconv3_1/unpooling\n",
    "        unpool_3 = torch.nn.MaxUnpool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # deconv3_2\n",
    "        deconv3_2 = torch.nn.ConvTranspose2d(in_channels = 256, \n",
    "                             out_channels = 128, \n",
    "                             kernel_size = [5, 5])\n",
    "        batch_norm_3 = torch.nn.BatchNorm2d(num_features = 128)\n",
    "        # deconv2_1/unpooling\n",
    "        unpool_2 = torch.nn.MaxUnpool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # deconv2_2\n",
    "        deconv2_2 = torch.nn.ConvTranspose2d(in_channels = 128, \n",
    "                             out_channels = 64, \n",
    "                             kernel_size = [5, 5])\n",
    "        batch_norm_2 = torch.nn.BatchNorm2d(num_features = 64)\n",
    "        # deconv1_1/unpooling\n",
    "        unpool_1 = torch.nn.MaxUnpool2d(kernel_size = [2, 2], \n",
    "                       stride = [2, 2])\n",
    "        # deconv1_2    \n",
    "        deconv1_2 = torch.nn.ConvTranspose2d(in_channels = 64, \n",
    "                             out_channels = 64, \n",
    "                             kernel_size = [5, 5])\n",
    "        batch_norm_1 = torch.nn.BatchNorm2d(num_features = 64)\n",
    "        # pred_alpha_matte\n",
    "        pred_alpha_matte = torch.nn.Conv2d(in_channels = 64, \n",
    "                        out_channels = 1, \n",
    "                        kernel_size = [5, 5])\n",
    "        act_sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "                                 \n",
    "                                 \n",
    "        \n",
    "    def forward(self, b_input):\n",
    "        # conv1_1  \n",
    "        x = conv1_1(b_input)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # conv1_2\n",
    "        x = conv1_2(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # pool1\n",
    "        x = pool1(x)\n",
    "            \n",
    "        # conv2_1\n",
    "        x = conv2_1(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # conv2_2\n",
    "        x = conv2_2(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # pool2\n",
    "        x = pool2(x)\n",
    "            \n",
    "        # conv3_1\n",
    "        x = conv3_1(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # conv3_2\n",
    "        x = conv3_2(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # conv3_3\n",
    "        x = conv3_3(x)\n",
    "        x = actiovation(x)\n",
    "            \n",
    "        # pool3\n",
    "        x = pool3(x)\n",
    "\n",
    "        # conv4_1\n",
    "        x = conv4_1(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # conv4_2\n",
    "        x = conv4_2(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # conv4_3\n",
    "        x = conv4_3(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # pool4\n",
    "        x = pool4(x)\n",
    "        \n",
    "        # conv5_1\n",
    "        x = conv5_1(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # conv5_2\n",
    "        x = conv5_2(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # conv5_3\n",
    "        x = conv5_3(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        # pool5\n",
    "        x = pool5(x)\n",
    "        \n",
    "        # conv6_1 \n",
    "        x = conv6_1(x)\n",
    "        x = actiovation(x)\n",
    "        \n",
    "        \n",
    "        # deconv6\n",
    "        x = deconv6(x)\n",
    "        x = batch_norm_6(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        #deconv5_1/unpooling\n",
    "        x = unpool_5(x)\n",
    "                                            \n",
    "        # deconv5_2\n",
    "        x = deconv5_2(x)\n",
    "        x = batch_norm_5(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        # deconv4_1/unpooling)\n",
    "        x = unpool_4(x)\n",
    "        \n",
    "        # deconv4_2\n",
    "        x = deconv4_2(x)\n",
    "        x = batch_norm_4(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        # deconv3_1/unpooling\n",
    "        x = unpool_3(x)\n",
    "        \n",
    "        # deconv3_2\n",
    "        x = deconv3_2(x)\n",
    "        x = batch_norm_3(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        # deconv2_1/unpooling\n",
    "        x = unpool_2(x)\n",
    "        \n",
    "        # deconv2_2\n",
    "        x = deconv2_2(x)\n",
    "        x = batch_norm_2(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        # deconv1_1/unpooling\n",
    "        x = unpool_1(x)\n",
    "        \n",
    "        # deconv1_2  \n",
    "        x = deconv1_2(x)\n",
    "        x = batch_norm_1(x)\n",
    "        x = activation(x)\n",
    "        \n",
    "        # pred_alpha_matte\n",
    "        x = pred_alpha_matte(x)\n",
    "        pred_mattes = act_sigmoid(x)\n",
    "        \n",
    "        return pred_mattes\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> create net</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>define loss and optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss():\n",
    "    wl = torch.where(torch.equal(b_trimap, 128), \n",
    "                 torch.Tensor([train_batch_size,image_size,image_size,1]).fill(1.),\n",
    "                 torch.Tensor([train_batch_size,image_size,image_size,1]).fill(0.))\n",
    "\n",
    "    unknown_region_size = torch.sum(wl)\n",
    "\n",
    "    alpha_diff = torch.sqrt((pred_mattes - GT_matte_batch)**2 + 1e-12)\n",
    "\n",
    "    p_RGB = []\n",
    "\n",
    "    b_GTBG = b_GTBG.view([train_batch_size,image_size,image_size,3])\n",
    "    b_GTFG = b_GTFG.view([train_batch_size,image_size,image_size,3])\n",
    "\n",
    "    raw_RGBs = raw_RGBs.view([train_batch_size,image_size,image_size,3])\n",
    "    b_GTmatte = b_GTmatte.view([train_batch_size,image_size,image_size,1])\n",
    "    \n",
    "    pred_final = torch.where(torch.equal(b_trimap, 128), , pred_mattes,b_trimap/255.0)\n",
    "\n",
    "    l_matte = torch.unbind(pred_final)\n",
    "    BG = torch.unbind(b_GTBG)\n",
    "    FG = torch.unbind(b_GTFG)\n",
    "\n",
    "    for i in range(train_batch_size):\n",
    "        p_RGB.append(l_matte[i] * FG[i] + (1. - l_matte[i]) * BG[i])\n",
    "    pred_RGB = torch.stack(p_RGB)\n",
    "\n",
    "    c_diff = torch.sqrt((pred_RGB - raw_RGBs)**2 + 1e-12)/255.0\n",
    "\n",
    "    alpha_loss = torch.sum(alpha_diff * wl)/(unknown_region_size)\n",
    "    comp_loss = torch.sum(c_diff * wl)/(unknown_region_size)\n",
    "\n",
    "    total_loss = (alpha_loss + comp_loss) * 0.5\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-5)\n",
    "criterion = loss()\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    \n",
    "    while epoch_num < max_epochs:  \n",
    "        while batch_num < batchs_per_epoch:\n",
    "            batch_alpha_paths = paths_alpha[batch_index]\n",
    "            batch_eps_paths = paths_eps[batch_index]\n",
    "            batch_BG_paths = paths_BG[batch_index]\n",
    "            batch_RGBs,batch_trimaps,batch_alphas,batch_BGs,batch_FGs,RGBs_with_mean = load_data(batch_alpha_paths,batch_eps_paths,batch_BG_paths)\n",
    "\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(config=tf.ConfigProto(gpu_options = gpu_options)) as sess:\n",
    "\n",
    "    while epoch_num < max_epochs:  \n",
    "        while batch_num < batchs_per_epoch:\n",
    "\n",
    "            batch_alpha_paths = paths_alpha[batch_index]\n",
    "            batch_eps_paths = paths_eps[batch_index]\n",
    "            batch_BG_paths = paths_BG[batch_index]\n",
    "            batch_RGBs,batch_trimaps,batch_alphas,batch_BGs,batch_FGs,RGBs_with_mean = load_data(batch_alpha_paths,batch_eps_paths,batch_BG_paths)\n",
    "\n",
    "            feed = {image_batch:batch_RGBs, GT_matte_batch:batch_alphas,GT_trimap:batch_trimaps, GTBG_batch:batch_BGs, GTFG_batch:batch_FGs,raw_RGBs:RGBs_with_mean,training:True}\n",
    "\n",
    "            _,loss,summary_str,step= sess.run([train_op,total_loss,summary_op,global_step],feed_dict = feed)\n",
    "            print('epoch %d   batch %d   loss is %f' %(epoch_num,batch_num,loss))\n",
    "\n",
    "            if step%200 == 0:\n",
    "                print('saving model......')\n",
    "                saver.save(sess,'./model/model.ckpt',global_step = step, write_meta_graph = False)\n",
    "\n",
    "                print('test on validation data...')\n",
    "                test_RGBs,test_trimaps,test_alphas,all_shape,image_paths,trimap_size = load_alphamatting_data(test_dir)\n",
    "                vali_diff = []\n",
    "                \n",
    "                for i in range(len(test_RGBs)):\n",
    "                    test_RGB = np.expand_dims(test_RGBs[i],0)\n",
    "                    test_trimap = np.expand_dims(test_trimaps[i],0)\n",
    "                    test_alpha = test_alphas[i]\n",
    "                    shape_i = all_shape[i]\n",
    "                    image_path = image_paths[i]\n",
    "                    \n",
    "                    feed = {image_batch:test_RGB,GT_trimap:test_trimap,training:False}\n",
    "                    test_out = sess.run(pred_final,feed_dict = feed)\n",
    "                    \n",
    "                    i_out = misc.imresize(test_out[0,:,:,0],shape_i)\n",
    "                    vali_diff.append(np.sum(np.abs(i_out/255.0-test_alpha))/trimap_size[i])\n",
    "                    misc.imsave(os.path.join(test_outdir,image_path),i_out)\n",
    "                \n",
    "                vali_loss = np.mean(vali_diff)\n",
    "                print('validation loss is '+ str(vali_loss))\n",
    "                validation_summary = tf.Summary()\n",
    "                validation_summary.value.add(tag='validation_loss',simple_value = vali_loss)\n",
    "                summary_writer.add_summary(validation_summary,step)\n",
    "\n",
    "            summary_writer.add_summary(summary_str,global_step = step)\n",
    "            batch_num += 1\n",
    "        batch_num = 0\n",
    "        epoch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.add_to_collection(\"pred_mattes\", pred_mattes)\n",
    "\n",
    "wl = tf.where(tf.equal(b_trimap,128),tf.fill([train_batch_size,image_size,image_size,1],1.),tf.fill([train_batch_size,image_size,image_size,1],0.))\n",
    "unknown_region_size = tf.reduce_sum(wl)\n",
    "\n",
    "tf.summary.image('pred_mattes',pred_mattes,max_outputs = 5)\n",
    "alpha_diff = tf.sqrt(tf.square(pred_mattes - GT_matte_batch)+ 1e-12)\n",
    "\n",
    "p_RGB = []\n",
    "pred_mattes.set_shape([train_batch_size,image_size,image_size,1])\n",
    "b_GTBG.set_shape([train_batch_size,image_size,image_size,3])\n",
    "b_GTFG.set_shape([train_batch_size,image_size,image_size,3])\n",
    "raw_RGBs.set_shape([train_batch_size,image_size,image_size,3])\n",
    "b_GTmatte.set_shape([train_batch_size,image_size,image_size,1])\n",
    "\n",
    "pred_final =  tf.where(tf.equal(b_trimap,128), pred_mattes,b_trimap/255.0)\n",
    "tf.summary.image('pred_final',pred_final,max_outputs = 5)\n",
    "\n",
    "l_matte = tf.unstack(pred_final)\n",
    "BG = tf.unstack(b_GTBG)\n",
    "FG = tf.unstack(b_GTFG)\n",
    "\n",
    "for i in range(train_batch_size):\n",
    "    p_RGB.append(l_matte[i] * FG[i] + (tf.constant(1.) - l_matte[i]) * BG[i])\n",
    "pred_RGB = tf.stack(p_RGB)\n",
    "\n",
    "tf.summary.image('pred_RGB',pred_RGB,max_outputs = 5)\n",
    "c_diff = tf.sqrt(tf.square(pred_RGB - raw_RGBs) + 1e-12)/255.0\n",
    "\n",
    "alpha_loss = tf.reduce_sum(alpha_diff * wl)/(unknown_region_size)\n",
    "comp_loss = tf.reduce_sum(c_diff * wl)/(unknown_region_size)\n",
    "\n",
    "# tf.summary.image('alpha_diff',alpha_diff * wl_alpha,max_outputs = 5)\n",
    "# tf.summary.image('c_diff',c_diff * wl_RGB,max_outputs = 5)\n",
    "\n",
    "tf.summary.scalar('alpha_loss',alpha_loss)\n",
    "tf.summary.scalar('comp_loss',comp_loss)\n",
    "\n",
    "total_loss = (alpha_loss + comp_loss) * 0.5\n",
    "tf.summary.scalar('total_loss',total_loss)\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate = 1e-5).minimize(total_loss,global_step = global_step)\n",
    "\n",
    "saver = tf.train.Saver(tf.trainable_variables() , max_to_keep = 1)\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.6)\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options = gpu_options)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.train.start_queue_runners(coord=coord,sess=sess)\n",
    "    batch_num = 0\n",
    "    epoch_num = 0\n",
    "    #initialize all parameters in vgg16\n",
    "    if not pretrained_model:\n",
    "        weights = np.load(model_path)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            if i == 28:\n",
    "                break\n",
    "            if k == 'conv1_1_W':  \n",
    "                sess.run(en_parameters[i].assign(np.concatenate([weights[k],np.zeros([3,3,1,64])],axis = 2)))\n",
    "            else:\n",
    "                if k=='fc6_W':\n",
    "                    tmp = np.reshape(weights[k],(7,7,512,4096))\n",
    "                    sess.run(en_parameters[i].assign(tmp))\n",
    "                else:\n",
    "                    sess.run(en_parameters[i].assign(weights[k]))\n",
    "        print('finish loading vgg16 model')\n",
    "    else:\n",
    "        print('Restoring pretrained model...')\n",
    "        saver.restore(sess,tf.train.latest_checkpoint('./model'))\n",
    "    sess.graph.finalize()\n",
    "\n",
    "    while epoch_num < max_epochs:  \n",
    "        while batch_num < batchs_per_epoch:\n",
    "            batch_index = sess.run(index_dequeue_op)\n",
    "\n",
    "            batch_alpha_paths = paths_alpha[batch_index]\n",
    "            batch_eps_paths = paths_eps[batch_index]\n",
    "            batch_BG_paths = paths_BG[batch_index]\n",
    "            batch_RGBs,batch_trimaps,batch_alphas,batch_BGs,batch_FGs,RGBs_with_mean = load_data(batch_alpha_paths,batch_eps_paths,batch_BG_paths)\n",
    "\n",
    "            feed = {image_batch:batch_RGBs, GT_matte_batch:batch_alphas,GT_trimap:batch_trimaps, GTBG_batch:batch_BGs, GTFG_batch:batch_FGs,raw_RGBs:RGBs_with_mean,training:True}\n",
    "\n",
    "            _,loss,summary_str,step= sess.run([train_op,total_loss,summary_op,global_step],feed_dict = feed)\n",
    "            print('epoch %d   batch %d   loss is %f' %(epoch_num,batch_num,loss))\n",
    "\n",
    "            if step%200 == 0:\n",
    "                print('saving model......')\n",
    "                saver.save(sess,'./model/model.ckpt',global_step = step, write_meta_graph = False)\n",
    "\n",
    "                print('test on validation data...')\n",
    "                test_RGBs,test_trimaps,test_alphas,all_shape,image_paths,trimap_size = load_alphamatting_data(test_dir)\n",
    "                vali_diff = []\n",
    "                \n",
    "                for i in range(len(test_RGBs)):\n",
    "                    test_RGB = np.expand_dims(test_RGBs[i],0)\n",
    "                    test_trimap = np.expand_dims(test_trimaps[i],0)\n",
    "                    test_alpha = test_alphas[i]\n",
    "                    shape_i = all_shape[i]\n",
    "                    image_path = image_paths[i]\n",
    "                    \n",
    "                    feed = {image_batch:test_RGB,GT_trimap:test_trimap,training:False}\n",
    "                    test_out = sess.run(pred_final,feed_dict = feed)\n",
    "                    \n",
    "                    i_out = misc.imresize(test_out[0,:,:,0],shape_i)\n",
    "                    vali_diff.append(np.sum(np.abs(i_out/255.0-test_alpha))/trimap_size[i])\n",
    "                    misc.imsave(os.path.join(test_outdir,image_path),i_out)\n",
    "                \n",
    "                vali_loss = np.mean(vali_diff)\n",
    "                print('validation loss is '+ str(vali_loss))\n",
    "                validation_summary = tf.Summary()\n",
    "                validation_summary.value.add(tag='validation_loss',simple_value = vali_loss)\n",
    "                summary_writer.add_summary(validation_summary,step)\n",
    "\n",
    "            summary_writer.add_summary(summary_str,global_step = step)\n",
    "            batch_num += 1\n",
    "        batch_num = 0\n",
    "        epoch_num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
